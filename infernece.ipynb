{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "from pathlib import Path\n",
    "\n",
    "from TPTBox import NII\n",
    "\n",
    "from inference_help import upscale_nii\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superresolution\n",
    "\n",
    "\n",
    "You can download a pretrained model here:\n",
    "[SR_axial_to_sag](https://studentpartnersde-my.sharepoint.com/:f:/g/personal/robert_graf_studentpartners_de/ErqaSmNSB8xMub9xe6R9U3wBiNIPFy37Kwb5z5fTh3pp2g?e=BtzNCI)\n",
    "\n",
    "Note: As descried in the paper we only improve the sagittal resolution. A second phase would be needed to fix the artifact in the other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_checkpoint = \"lightning_logs_dae/my_dataset/version_0/checkpoints/last.ckpt\"\n",
    "if Path(\"lightning_logs_dae/pretrained_ax2sag/version_0/checkpoints/last.ckpt\").exists():\n",
    "    \n",
    "    default_checkpoint=\"lightning_logs_dae/pretrained_ax2sag/version_0/checkpoints/last.ckpt\"\n",
    "\n",
    "#lr_img = NII.load('path/to/nii.nii.gz',False)\n",
    "lr_img = NII.load(\"/media/data/robert/datasets/dataset-neuropoly-test/rawdata/sub-m073580/ses-20130313/anat/sub-m073580_ses-20130313_acq-ax_chunk-1_T2w.nii.gz\",False)\n",
    "out_path =Path(\"sub-x_acq-iso_T2w.nii.gz\")\n",
    "hr_img = upscale_nii(lr_img,out_path,batch_size=32,checkpoint_sag=default_checkpoint,override_upscale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "Install the segmentation from \n",
    "https://github.com/Hendrik-code/spineps\n",
    "\n",
    "and follow the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from mri_segmentor import get_model_spine, get_model_vert\n",
    "\n",
    "#def segment(\n",
    "#    nii: NII,\n",
    "#    raw=\"rawdata\",\n",
    "#    der=\"derivatives\",\n",
    "#    override_upscale=False,\n",
    "#    batch_size=32,\n",
    "#    sort=True,\n",
    "#    device=torch.device(\"cuda:0\"),  # noqa: B008\n",
    "#    resample_back=True,\n",
    "#    sag_only=False,\n",
    "#):\n",
    "#    # INPUT\n",
    "#    in_ds = Path(in_ds)\n",
    "#    head_logger = No_Logger()  # (in_ds, log_filename=\"source-convert-to-unet-train\", default_verbose=True)#\n",
    "\n",
    "#    block = \"\"  # put i.e. 101 in here for block\n",
    "#    parent_raw = str(Path(raw).joinpath(str(block)))\n",
    "#    parent_der = str(Path(der).joinpath(str(block)))\n",
    "#    from mri_segmentor import get_model_spine, get_model_vert#\n",
    "\n",
    "#    # check available models\n",
    "#    # modelid2folder_subreg, modelid2folder_vert = check_available_models(model_dir, verbose=True)\n",
    "#    model_subreg = get_model_spine(\"T2w_Segmentor\").load()\n",
    "#    try:\n",
    "#        model_vert = get_model_vert(\"vert_highres\").load()\n",
    "#    except KeyError:\n",
    "#        model_vert = get_model_vert(\"Vertebra_Highres\").load()#\n",
    "\n",
    "#    BIDS_Global_info.remove_splitting_key(\"chunk\")\n",
    "#    bids_ds = BIDS_Global_info(datasets=[in_ds], parents=[parent_raw, parent_der], verbose=False)#\n",
    "\n",
    "#    execution_times = []\n",
    "#    for name, subject in bids_ds.enumerate_subjects(sort=True):\n",
    "#        logger = head_logger.add_sub_logger(name=name)\n",
    "#        q = subject.new_query()\n",
    "#        q.flatten()\n",
    "#        q.filter(\"part\", \"inphase\", required=False)\n",
    "#        # q.filter(\"acq\", \"ax\")\n",
    "#        q.filter(\"seg\", lambda x: x != \"manual\", required=False)\n",
    "#        q.filter(\"lesions\", lambda x: x != \"manual\", required=False)\n",
    "#        # q.filter(\"desc\", lambda _: False, required=False)\n",
    "#        q.unflatten()\n",
    "#        q.filter_format(\"T2w\")\n",
    "#        q.filter_filetype(\"nii.gz\")\n",
    "#        families = q.loop_dict(sort=sort, key_addendum=[\"acq\"])\n",
    "#        for f in families:\n",
    "#            print(f)\n",
    "#            try:\n",
    "#                fid = f.family_id\n",
    "#                if \"T2w_acq-sag\" in f:\n",
    "#                    for t2w in f[\"T2w_acq-sag\"]:\n",
    "#                        start_time = time.perf_counter()\n",
    "#                        reduce_nii_size(t2w, t2w.open_nii())\n",
    "#                        # Call to the pipeline\n",
    "#                        output_paths, errcode = process_img_nii(\n",
    "#                            img_ref=t2w,\n",
    "#                            derivative_name=der,\n",
    "#                            model_subreg=model_subreg,\n",
    "#                            model_vert=model_vert,\n",
    "#                            override_subreg=False,\n",
    "#                            override_vert=False,\n",
    "#                            save_debug_data=False,\n",
    "#                            verbose=False,\n",
    "#                        )\n",
    "#                        end_time = time.perf_counter()\n",
    "#                        execution_time = end_time - start_time\n",
    "#                        logger.print(f\"Inference time is: {execution_time}\")\n",
    "#                        execution_times.append(execution_time)\n",
    "#                        if errcode == ErrCode.UNKNOWN:\n",
    "#                            continue\n",
    "#                        if errcode not in [ErrCode.OK, ErrCode.ALL_DONE]:\n",
    "#                            logger.print(f\"{fid}: Pipeline threw error code {errcode}\")\n",
    "#                if \"T2w_acq-ax\" not in f or sag_only:\n",
    "#                    # logger.print(f\"{fid}: T2w without part- not found, skip\")\n",
    "#                    continue\n",
    "#                for t2w in f[\"T2w_acq-ax\"]:\n",
    "#                    start_time = time.perf_counter()\n",
    "#                    t2w_nii = t2w.open_nii()\n",
    "#                    t2w_nii = reduce_nii_size(t2w, t2w_nii)\n",
    "#                    nii_iso, path_iso = upscale_nii(\n",
    "#                        nii_org_bids=t2w, parent=iso, override_upscale=override_upscale, batch_size=batch_size, device=device\n",
    "#                    )\n",
    "#                    # Call to the pipeline\n",
    "#                    output_paths, errcode = process_img_nii(\n",
    "#                        img_ref=BIDS_FILE(path_iso, t2w.dataset),\n",
    "#                        derivative_name=der,\n",
    "#                        model_subreg=model_subreg,\n",
    "#                        model_vert=model_vert,\n",
    "#                        override_subreg=False,\n",
    "#                        override_vert=False,\n",
    "#                        lambda_subreg=filter_segmentation,\n",
    "#                        save_debug_data=False,\n",
    "#                        verbose=False,\n",
    "#                    )\n",
    "#                    end_time = time.perf_counter()\n",
    "#                    execution_time = end_time - start_time\n",
    "#                    logger.print(f\"Inference time is: {execution_time}\")\n",
    "#                    execution_times.append(execution_time)\n",
    "#                    if errcode == ErrCode.UNKNOWN:\n",
    "#                        continue\n",
    "#                    if errcode not in [ErrCode.OK, ErrCode.ALL_DONE]:\n",
    "#                        logger.print(f\"{fid}: Pipeline threw error code {errcode}\")\n",
    "#                    # Load Outputs\n",
    "#                    seg_nii = NII.load(output_paths[\"out_spine\"], seg=True)  # subregion mask\n",
    "#                    vert_nii = NII.load(output_paths[\"out_vert\"], seg=True)  # subregion mask\n",
    "#                    poi = calc_centroids_from_subreg_vert(\n",
    "#                        vert_nii,\n",
    "#                        seg_nii,\n",
    "#                        subreg_id=[50, 100, Location.Spinal_Canal_ivd_lvl.value, Location.Spinal_Canal.value],\n",
    "#                        buffer_file=output_paths[\"out_ctd\"],\n",
    "#                        save_buffer_file=True,\n",
    "#                    )\n",
    "#                    ### RESCALE BACK ###\n",
    "#                    if resample_back:\n",
    "#                        output_paths = output_paths_from_input(t2w, der, None)\n",
    "#                        out_spine = output_paths[\"out_spine\"]\n",
    "#                        out_vert = output_paths[\"out_vert\"]\n",
    "#                        out_ctd = output_paths[\"out_ctd\"]\n",
    "#                        if not out_spine.exists() or override_upscale:\n",
    "#                            seg = seg_nii.map_labels({Location.Endplate.value: Location.Vertebra_Disc.value})\n",
    "#                            seg.resample_from_to_(t2w_nii).save(out_spine)\n",
    "#                        if not out_vert.exists() or override_upscale:\n",
    "#                            seg = vert_nii.map_labels({i: i % 100 + 100 for i in range(200, 500)})\n",
    "#                            seg.resample_from_to_(t2w_nii).save(out_vert)\n",
    "#                        if not out_ctd.exists() or override_upscale:\n",
    "#                            poi_lr = poi.resample_from_to(t2w_nii)\n",
    "#                            poi_lr.save(out_ctd)\n",
    "#                        # vertebra_level = BIDS_FILE(output_paths[\"out_spine\"], t2w.dataset).get_changed_path(info={\"seg\": \"vertebra-level\"})#\n",
    "\n",
    "#                        # poi_lr.extract_subregion(Location.Spinal_Canal_ivd_lvl.value).make_point_cloud_nii(s=4)[0].save(vertebra_level)\n",
    "#            except Exception:\n",
    "#                logger.print_error()\n",
    "#    if len(execution_times) > 0:\n",
    "#        head_logger.print(\n",
    "#            f\"\\nExecution times:\\n{execution_times}\\nRange:{min(execution_times)}, {max(execution_times)}\\nAvg {np.average(execution_times)}\"\n",
    "#        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
